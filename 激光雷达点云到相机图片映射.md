# 激光雷达点云到相机图片映射



<br>

**本文为我个人实际点云标注工具开发遇到的一些经验总结，如何实现：将点云场景中标注的立方体映射到相机图片上**

本文所说的：

* "点云场景" ：是指在浏览器中使用 three.js 加载 自动驾驶中激光雷达得到的 .pcd 文件并渲染出来的 3D 空间
* "相机图片" ：是指车载相机所拍摄的照片
* "映射"：是指 3D 点云场景中绘制的立方体对应在相机中的对应的映射结果，即伪立方体或框



<br>

**实现起来很简单：就是把点云(3D)中的某个点(vector3)转换成相机的坐标(2D)，即 2 个坐标系之间的空间转换。**

只不过对于新手而言，这里面有很多前置知识不了的话，刚开始并不好理出头绪。



<br>

**本文目录：**

* 点云的坐标系与three.js坐标系的差异
* 相机的内参和外参
* 点云3D空间中某个点映射到相机2D图片中
* 新的映射算法
* 关于点云BEV(鸟瞰图)的映射



<br>

> 补充一个和本文并无关系的知识点：
>
> **计算 3D 某个点在相机(正交或透视)中的渲染结果画面中的位置：**
>
> * 假定 3D 点坐标为 vector3
> * 假定相机为 camera
> * 那么计算该点在 相机中的渲染结果为：`vector3.project(camera)`
>
> 反之，将相机渲染结果画面中某个点 `resVector3` 映射回 3D 空间中的点坐标，对应的计算方式为：`resVector3.unproject(camera)`



<br>

## 点云的坐标系与three.js坐标系的差异



<br>

**原始点云与three.js坐标系的不同：**

* three.js 中采用右手坐标系，在我们的相机默认视角中
  * 屏幕右侧为 X 轴正方向
  * 屏幕向上为 Y 轴正方向
  * 屏幕指向我们眼睛的为 Z 轴正方向 

* 自动驾驶的点云坐标系，通常为
  * 汽车车头方向为 X 轴正方向
  * 汽车车顶方向为 Y 轴正方向
  * 汽车左侧方向为 Z 轴正方向

由于点云与 three.js 空间坐标轴方向不同，所以如果默认加载进来的点云通常是 "头朝下" 显示的。



<br>

**坐标系之间的变换矩阵：**

如果你不介意点云方向不正确那也无所谓，可以跳过本小节。

如果你希望能够能够将点云矫正过来，那么你需得到这 2 个坐标系(原始点云与three.js)之间的转换矩阵。

假定在 three.js 的空间中，我们将点云进行下面的旋转操作就可以将点云 "方向纠正"：

* 将原始点云 X 轴旋转 -90度
* Y 轴保持不变
* Z 轴旋转 180度

<br>

> 上面提到的旋转角度 正或负，遵循的是右手螺旋法则



<br>

注意上面旋转角度仅仅是我们举例，实际点云旋转能否纠正还要看具体情况。

同时 three.js 使用的是弧度而不是度，不管如何，我们假定当前的旋转为：

* x 轴旋转 xrn (弧度) //该值可以是 0
* y 轴旋转 yrn (弧度) //该值可以是 0
* z 轴旋转 zrn (弧度) //该值可以是 0

有了上面 3 个值，就可以计算出对应的旋转矩阵 rotationMatrix：

```
import { Euler, Matrix4 } from "three"

const rotationMatrix = new Matrix4().makeRotationFromEuler(new Euler(xrn, yrn, zrn))
```



<br>

实际中还有可能需要进行 "点云平移"。

假定：

* x 轴平移 xtn (米) //该值可以是 0
* y 轴平移 ytn (米) //该值可以是 0
* z 轴平移 ztn (米)  //该值可以是 0

有了上面 3 个值，我们就可以计算出对应的平移矩阵 offsetMatrix：

```
const offsetMatrix = new Matrix4().makeTranslation(xtn, ytn, ztn)
```



<br>

那我们此时有了 平移矩阵 和 旋转矩阵，就可以得出 点云 在 three.js 中进行的变换矩阵 pcdMatrix：

```
const pcdMatrix = offsetMatrix.clone().multiply(rotationMatrix)
```

> 注：我们使用了 平移矩阵乘以旋转矩阵，意味着其结果是：先执行旋转，然后再平移

知道了 pcdMatrix 自然就可以知道它的逆矩阵 pcdMatrixInvert：

```
const pcdMatrixInvert = pcdMatrix.clone().invert()
```



<br>

只此，我们知道了 点云在 three.js 中的变换矩阵 pcdMatrix 和它的逆矩阵 pcdMatrixInvert。



<br>

**执行"点云矫正"：**

为了改变点云显示结果，对其 "矫正"，我们只需在 PCDLoader 加载点云完成事件中操作一下：

```
import { PCDLoader } from 'three/examples/jsm/loaders'

const pcdLoader = new PCDLoader()
pcdLoader.load('xxx.pcd', (points) =>{
    points.geometry.applyMatrix4(pcdMatrix)
    
    ...
    
    editor.scene.add(points)
})
```

这样点云的方向就被我们 "矫正" 过来了。



<br>

再次重申：本小节所讲 "点云矫正"，仅仅实际项目中可能发生的，但不是必须的，你完全可以忽略，跳过本小节。



<br>

## 相机的内参和外参



<br>

**什么是相机的内参(intrinsic)？**

这里说的相机是指物理上的相机，不是指 three.js 中的相机。

物理上的相机本质是基于小孔成像原理。

相机内参就是用来描述相机小孔成像中的一些参数：

* 焦距：2 个数值 fx、fy
* 原点：2 个数字 cx、cy
* 扭曲因子：通常情况下该值为 0，我们暂时用 s 代替

以上这些参数会构成一个 3x3 的矩阵：

```
//行优先
const arr1 = [fx,s,cx,0,fy,cy,0,0,1]

//列优先
const arr2 = [fx,0,0,s,fy,0,cx,cy,1]
```

对应 three.js 中的 3x3 矩阵：

```
const intrinsic: Matrix3 = new Matrix3()
intrinsic.fromArray(arr2)
```



<br>

**你需要谨记以下几点：**

* 相机内参是一个 3x3矩阵，在使用相机内参前一定注意查看这个矩阵是 行优先 还是 列优先
* 相机内参是该相机出厂前就固定下来的，不会随着自动驾驶车辆安装位置不同而发生任何变化



<br>

**什么是相机外参(extrinsics)？**

相机外参就是相机相对于整个车辆坐标系中心点(雷达)的变换参数。

也就是当前相机坐标与车辆中心点(雷达)坐标的变换转换。

因为自动驾驶车辆可以在不同方向安装多个相机，因此需要将这些不同相机都通过外参转化成共同、通用的车辆中心点(雷达)的坐标系上。



<br>

相机外参(extrinsics)的变化中只存在 平移和旋转，不存在缩放。

**相机外参是由以下几个参数沟通：**

* 平移：x轴平移参数、y轴平移参数、z轴平移参数
* 旋转：x轴旋转 roll(滚翻)、y轴旋转参数 pitch(俯仰)、z轴旋转参数 yaw(偏航)



<br>

**外参矩阵：**

相机外参是一个 4x4 的矩阵

我们假定：

* 平移参数为：x,y,z
* 旋转参数为：roll, pitch, yaw

我们知道这个矩阵不需要考虑缩放，那么知道平移和旋转，就可以在 three.js 中通过下面代码计算出该矩阵：

```
const positon = new Vector3(x, y, z)
const euler = new Euler(roll, pitch, yaw)
const quaternion = new Quaternion().setFromEuler(euler)
const scale = new Vector3(1, 1, 1)
const extrinsic = new Matrix4()
extrinsic.compose(positon, quaternion, scale)
```

> extrinsic 就是我们得到的外参矩阵，并且 three.js 矩阵采用的是 行优先



<br>

**相机外参是由 `相机与激光雷达标定` 得到的，在使用客户给你提供的相机外参时一定要注意这样一件事：**

**客户提供的外参究竟是 雷达到相机的矩阵 还是 相机到雷达的矩阵？**

**如果客户提供的外参是 雷达到相机，那么我们实际需要的是该矩阵的逆矩阵，即 相机到雷达的转换矩阵。**

```
extrinsic = extrinsic.invert()
```



<br>

**相机的最终转换矩阵：相机内参矩阵 + 外参矩阵**

```
const cameraMatrix = new Matrix4().setFromMatrix3(intrinsic).multiply(extrinsic)
```

> 再次提醒：上面代码中的相机外参矩阵 extrinsic 是指相机到雷达(点云中心点) 的变换矩阵
>
> 如果客户提供的 extrinsic 矩阵是雷达到相机，那么上面代码就需要改成：
>
> ```
> const cameraMatrix = new Matrix4().setFromMatrix3(intrinsic).multiply(extrinsic.invert())
> ```



<br>

好了只此，我们得到了相机的变换矩阵，距离映射又进了一步。



<br>

**如果你想对相机内参外参有进一步深入了解，推荐查看下面 2 个教程：**

* https://ksimek.github.io/2013/08/13/intrinsic/
* https://www.uio.no/studier/emner/matnat/its/nedlagte-emner/UNIK4690/v16/forelesninger/lecture_1_4-the-perspective-camera-model.pdf



<br>

------------ 以下更新于 2024.01.05 -----------

### 特别说明

**下面要讲解的关于 "点云3D空间中某个点映射到相机2D图片中" 的内容是之前的映射算法，会存在以下 2 个问题：**

1. 假设 3D 的某个点位于相机背面，那么如果直接让 resPoint.x 和 resPoint.y 分别除以 `resPoint.z` 会让最终的映射结果错误

   > 对于处于相机前方的点映射结果不会有任何问题

2. 假定 `resPoint.z` 足够小，比如其值为 0.0001，那么会造成计算出的 x 和 y 值足够大，也不是我们希望的

所以，下面的算法 **将 3D 点直接应用相机变换矩阵，在实际中会存在一些异常情况，因为这种直接应用矩阵的方式只是数学上理论的计算结果，并没有给我们一些修正某些极端情况的机会。**

**极端情况就是上面讲到的：**

* 3D 点处于相机后方：.z 的值为负数
* 3D 点的 .z 值足够小：例如其值为 0.001，造成 .x 和 .y 异常过大



<br>

> 假定立方体 8 个顶点有一些处于相机前方，有一些出于相机后方，此时依然要准确得到映射就结果，之前的算法就无法正确处理。



<br>

感谢 ChatGPT 提供了一个新的映射算法，我会将新的映射算法写到本文后面。

但是我依然强烈建议你认真阅读下面的 "点云3D空间中某个点映射到相机2D图片中" 章节，因为里面讲解的整个坐标转换过程本身是没有问题的，只是最终一点点映射算法不一样而已。

如果你不阅读直接看本文最后面新的映射算法，你可能看不懂的。

------------ 以上更新于 2024.01.05 -----------



<br>

## 点云3D空间中某个点映射到相机2D图片中



<br>

**已有数据回顾：**

点云变换相关：

* pcdMatrix：点云加载到 three.js 中为了方便查看而进行的变换矩阵，4x4矩阵
* pcdMatrixInvert：pcdMatrix 矩阵的逆矩阵
* 如果你根本没有进行过点云旋转，那么可以把 pcdMatrix 理解成一个没有经过任何变换的 4x4矩阵

相机内外参：

* intrinsic：相机内参，3x3矩阵
* extrinsic：相机外参，4x4矩阵
* cameraMatrix：相机矩阵，由内参和外参共同作用下的矩阵，4x4矩阵



<br>

**假定当前 3D 点云空间中有一个点 point 坐标为 vector3(x,y,z)，那么这个点在某相机中的空间投影变换就是：**

```
const resPoint = point.clone() //为了避免修改 point 我们克隆出一份
resPoint.applyMatrix4(pcdMatrixInvert) //先将该点由 three.js 空间转换到点云原始空间坐标
resPoint.applyMatrix4(cameraMatrix) //再转换到相机坐标系中
```

> 如果点云从未发生过任何变换，那么可以忽略上面代码中的 `resPoint.applyMatrix4(pcdMatrixInvert)`

请注意此时 resPoint 仅仅是转换到了相机坐标系中，但并不是相机拍摄照片的图片本身上。



<br>

我们还需进行下面的操作：

我们假定该相机最终拍摄照片尺寸为：1280 x 720

```
//定义一个 toFixed 函数，用于将一个数字仅保留小数点后8位，避免 JS 中的精度问题
const toFixed = (num: number, precision = 8) => {
    return Math.trunc(num * Math.pow(10, precision)) / Math.pow(10, precision);
}

const imgWidth = 1280
const imgHiehgt = 720

const isFront = resPoint.z > 0
if(isFront){
    resPoint.set(toFixed(resPoint.x / resPoint.z), toFixed(resPoint.y / resPoint.z), 1)
    const inView = resPoint.x > 0 && resPoint.x < imgWidth && resPoint.y > 0 && resPoint.y < imgHiehgt
    console.log(inView, resPoint.x, resPoint.y)
}else{
    console.log('point 不在相机拍照画面里')
}
```



**代码解释1：**

```
const isFront = resPoint.z > 0
```

* z > 0：表示该点最终位于相机的前面，才有可能被拍到
* z = 0：表示该点与相机的坐标 z 点相同，重叠，那么我们也认为这种情况下该点不应出现在相机拍摄画面中
* z < 0：表示该点位于相机背面，无需考虑，该点一定不可能出现在画面里(尽管该点可以映射到相机画面里)



<br>

**代码解释2：**

```
resPoint.set(toFixed(resPoint.x / resPoint.z), toFixed(resPoint.y / resPoint.z), 1)
```

我们把 resPoint 的坐标分量 x、y、z 都除以 z，姑且称之为 "z 坐标归一化"。

我们使用之前定义好的 toFixed() 将计算结果仅保留 8 位小数点。

经过此次转化 resPoint.x 和 resPoint.y 就是 3D 空间点 point 在该相机拍摄结果图片中的坐标。



<br>

> 但是这个坐标不一定在图片范围内，所以我们还需要下面那一行代码进行判断。



<br>

**代码解释3：**

```
const inView = resPoint.x > 0 && resPoint.x < imgWidth && resPoint.y > 0 && resPoint.y < imgHiehgt
```

我们判定 3D 空间中的点 point 在相机拍照结果图片中是否能够出现，依据就是：

* 该点位于相机前面
* 该点的 x 值在照片宽度范围内
* 该点的 y 值在照片高度范围内



<br>

**只此，3D空间中的点 point 最终在相机的拍照结果照片中的显示结果我们已经计算出来了 。**

* 当 inView 结果为 true，表示该点可以出现在相机画面里

* 该点在相机照片的坐标为 resPoint.x 和 resPoint.y

  > 在相机拍照图片中坐标系原始点位于左上角，resPoint.x 和 resPoint.y 表示距离左侧和顶部的位置



<br>

**以上就是一个 3D 点 映射到相机照片里的结果计算过程。**



<br>

**实际项目中通常是将 3D 空间中绘制的立方体映射到照片中，其实就是将该立方体的 8 个顶点坐标依次进行上面的计算，并最终得出这 8 个点在相机照片中的位置，这个过程就不再论述了。**



<br>

当你理解完这些前置知识和空间转换，你会发现整个过程极其简单。



<br>

------------ 以下更新于 2024.01.05 -----------

**下面是新的映射算法，解决上面讲解的一些可能存在的问题。**



<br>

## 新的映射算法



<br>

### 新的映射算法与之前的差异：

**第1步差异：应用的变换矩阵** 

* 之前是将 3D 点 applyMatrix4() 相机内参和外参 综合作用下的矩阵 cameraMatrix
* 而新算法是只应用相机外参矩阵 extrinsic

```diff
- resPoint.applyMatrix4(cameraMatrix)

+ resPoint.applyMatrix4(extrinsic)
```



<br>

**第2步差异：对于 z 值的处理**

* 之前没有考虑 z 值为 负情况下对于相机背面的异常处理、也没有考虑 z 值足够小的极端情况
* 而新算法对 z 值做了限定，我们将 z 进行截断限定在至少为 0.1

```diff
+ const z = Math.max(resPoint.z, 0.1)
```



<br>

**第3步差异：对于映射结果 x 和 y 的计算方式**

* 之前是让 .x 和 .y 简单粗暴直接除以 .z
* 而新算法是将 .x 和 .y 与相机内参中的 焦距(`fx、fy`)、原点(`cx、cy`) 按照某个公式进行求值

```diff
- resPoint.set(toFixed(resPoint.x / resPoint.z), toFixed(resPoint.y / resPoint.z), 1)

+ const z = Math.max(resPoint.z, 0.1)
+ const u = (fx * resPoint.x / z) + cx
+ const v = (fy * resPoint.y / z) + cy
+ resPoint.set(u, v, 1)
```



**补充说明：**

`fx、fy、cx、cy` 这 4 个值就是相机内参矩阵 intrinsic 中的

```
const fx = intrinsic.elements[0]
const fy = intrinsic.elements[4]
const cx = intrinsic.elements[6]
const cy = intrinsic.elements[7]
```



<br>

**当然最终判定映射结果是否在相机图片内容的那个判断无需修改。**

```
const inView = isFront && resPoint.x > 0 && resPoint.x < imgWidth && resPoint.y > 0 && resPoint.y < imgHiehgt
```

> 这部分代码逻辑是没有变化的



<br>

实际情况中就会发生一个立方体某些顶点在相机画面里，某些点在相机背面(不在相机画面里)这种情况，新的映射算法可以应对这种情况。



<br>

------------ 以下更新于 2024.01.08 -----------

## 关于点云BEV(鸟瞰图)的映射



<br>

### 什么是 BEV ？

BEV 就是单词 Brid's Eye View，即 鸟瞰图。

也可以称呼为：俯视图



<br>

**BEV图：**

实际中就是将若干个点云根据 各自的位姿矩阵(由惯导设备记录得到的) 合并成一个总体点云，然后使用正交相机，以俯视角度去渲染点云，得到的这个渲染结果就是 BEV 图。



<br>

**高度图：**

由于 BEV 图是已俯视视角去观察点云的，这里就缺少了高度信息。

所以在生成 BEV 图的同时，还需要生成一份像素点对应的高度图，用来记录每个 BEV 图上像素对应的地面高度信息。



<br>

具体如何生成 BEV 图和高度图，可以自己百度。



<br>

> 高度图中如何包含高度信息：
>
> 通常来说都是将每个像素的颜色值根据某种压缩比例对应高度信息。
>
> * 假定我们的高度信息取值范围为：-10米 ~ 10米
> * 而像素颜色取值范围为 0 ~ 255
>
> 那么我们需要：
>
> * 将 `-10 ~ 10` 中的一个数字转化为 `0 ~ 1` 的一个浮点数
> * 再将这个浮点数字 转化为 `0 ~ 255` 范围中对应的值
>
> 
>
> 一个数字在某个范围内所处的比例值换算公式：
>
> ```
> const res = (value - min) / (max - min) 
> ```
>
> 反过来，将某个比例值换算回具体范围的公式：
>
> ```
> const value = res * (max - min) + min
> ```
>
> 
>
> <br>
>
> 例如：5 米在 -10 米 ~ 10 米的范围对应的浮点数为：value 为 5、min 为 -10、max 为 10
>
> ```
> const res = ( 5 - (-10)) / (10 - (-10))
> ```
>
> 即：
>
> ```
> const res = 15 / 20 // 0.75
> ```
>
> 再将 0.75 转换到 0 ~  255 范围内：
>
> ```
> const color = Math.round(0.75 * 255)
> ```

> 可以看出这种转换过程是会产生一些损耗的，就看你是否接受这种误差。
>
> 如果你要求的特别精准，那么就别采用高度图用像素颜色值来保存高度信息这种方式，而是将高度信息以 .json 或 .txt 方式保存起来。



<br>

### 将 BEV 上的某个像素点映射到某一帧点云对应的相机中

假定我们在 BEV 图上有一个像素点：

1、该点在 BEV 图 上的 .x 值 和 .y 值

> 请注意这里隐含的信息是：.x 和 .y 都是整数

2、该点在 高度图 上的 .z 值

我们定义一个 Vector3 实例用来记录这些 .x、.y 和 .z 值

```
const bevPoint = new Vector3(x,y,z)
```



<br>

假定我们还知道下面的信息：

1、当前帧点云到 BEV 图的位姿转换矩阵的逆矩阵：poseMatrixInvert

2、当前帧各个相机的内参和外参



<br>

**那么将 BEV 上的某个像素点映射到某一帧点云对应的相机中，代码如下：**

```
//x,y 为该像素在 BEV 图上的像素位置
//z 为该像素在 高度图上 包含的高度值
const bevPoint = new Vector3(x,y,z)

//为了避免后续操作修改 bevPoint，所以我们克隆一份
const resPoint = bevPoint.clone()

//将 bev 中某个点转换到当前帧点云中对应的坐标
resPoint.applyMatrix4(poseMatrixInvert)

//应用当前相机的外参
resPoint.applyMatrix4(extrinsic)

//接下来的转换就和之前 点云中某个点 映射到某个相机的逻辑是相同了
const isFront = resPoint.z > 0

const fx = intrinsic.elements[0]
const fy = intrinsic.elements[4]
const cx = intrinsic.elements[6]
const cy = intrinsic.elements[7]
const z = Math.max(resPoint.z, 0.1)
const u = (fx * resPoint.x / z) + cx
const v = (fy * resPoint.y / z) + cy
resPoint.set(u, v, 1) //resPoints 中的 x,y 就是最终的映射结果
```



